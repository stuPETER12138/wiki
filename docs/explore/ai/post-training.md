# å¤§æ¨¡å‹çš„åè®­ç»ƒ

## Post-training

> ä¸¤ç¯‡ç»¼è¿°ï¼š[^4][^5]

![](./images/post_training.png)

å¯¹äºç§æœ‰åŒ–ï¼Œæˆ–æœ‰å‚ç›´è¡Œä¸šéœ€æ±‚çš„å¼€å‘è€…ï¼Œä¸€èˆ¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡ŒäºŒæ¬¡è®­ç»ƒï¼ˆå¾®è°ƒï¼Œå¯¹é½ç­‰ï¼‰ï¼Œåœ¨è®­ç»ƒåè¿›è¡Œè¯„æµ‹å’Œéƒ¨ç½²ã€‚ä»è®­ç»ƒè§’åº¦æ¥è¯´ï¼Œéœ€æ±‚ä¸€èˆ¬æ˜¯ï¼š

- å…·æœ‰å¤§é‡æœªæ ‡æ³¨è¡Œä¸šæ•°æ®ï¼Œéœ€è¦é‡æ–°è¿›è¡ŒCPTã€‚ä¸€èˆ¬ä½¿ç”¨Baseæ¨¡å‹è¿›è¡Œã€‚
- å…·æœ‰å¤§é‡é—®ç­”æ•°æ®å¯¹ï¼Œéœ€è¦è¿›è¡ŒSFTï¼Œæ ¹æ®æ•°æ®é‡é€‰ç”¨Baseæ¨¡å‹æˆ–Instructæ¨¡å‹è¿›è¡Œã€‚
- éœ€è¦æ¨¡å‹å…·å¤‡ç‹¬ç‰¹çš„å›å¤èƒ½åŠ›ï¼Œé¢å¤–åšä¸€æ¬¡RLHFã€‚
- éœ€è¦å¯¹æ¨¡å‹ç‰¹å®šé¢†åŸŸæ¨ç†èƒ½åŠ›ï¼ˆæˆ–æ€ç»´é“¾ï¼‰å¢å¼ºï¼Œä¸€èˆ¬ä¼šç”¨åˆ°è’¸é¦ã€é‡‡æ ·å¾®è°ƒæˆ–GRPO[^1]

### Fine-tuning

*å¾®è°ƒé¢„ä¼°æ˜¾å­˜æ¶ˆè€—*[^2]

| Methods                         | Bits | 7B     | 14B   | 30B   | `n`B    |
| ------------------------------- | ---- | ------ | ----- | ----- | ------- |
| Full (`bf16` or `fp16`)         | 32   | 120 GB | 240GB | 600GB | `18n`GB |
| Full (`pure_bf16`)              | 16   | 60 GB  | 120GB | 300GB | `8n`GB  |
| Freeze/Lora/GaLore/APOLLO/BAdam | 16   | 16 GB  | 32GB  | 64GB  | `2n`GB  |
| QLoRA                           | 8    | 10 GB  | 20GB  | 40GB  | `n`GB   |
| QLoRA                           | 4    | 6 GB   | 12GB  | 24GB  | `n/2`GB |
| QLoRA                           | 2    | 4 GB   | 8GB   | 16GB  | `n/4`GB |

### Reinforcement Learing

*GRPO å…¨é‡å¾®è°ƒæ˜¾å­˜éœ€æ±‚*[^3]

| Method                | Bits | 1.5B   | 3B     | 7B     | 32B     |
| --------------------- | ---- | ------ | ------ | ------ | ------- |
| GRPO Full Fine-Tuning | AMP  | 2*24GB | 4*40GB | 8*40GB | 16*80GB |
| GRPO Full Fine-Tuning | BF16 | 1*24GB | 1*40GB | 4*40GB | 8*80GB |

### è®­ç»ƒæ¡†æ¶

- [hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)](https://github.com/hiyouga/LLaMA-Factory)
- [modelscope/ms-swift: Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, GLM4, Mistral, Yi1.5, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, DeepSeek-VL2, Phi4, GOT-OCR2, ...).](https://github.com/modelscope/ms-swift)

- [volcengine/verl: verl: Volcano Engine Reinforcement Learning for LLMs](https://github.com/volcengine/verl)

## LoRA

![](./images/lora.png)

LoRA[^6]æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºå°†å¤§è¯­è¨€æ¨¡å‹é€‚åº”åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚LoRA æ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œ GPU å†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶å®ç°äº†ä¸å®Œå…¨å¾®è°ƒç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå®ƒä¸å¼•å…¥é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚

> å‡è®¾ï¼šé€‚åº”è¿‡ç¨‹ä¸­çš„æƒé‡æ›´æ–°å…·æœ‰è¾ƒä½çš„â€œå†…åœ¨ç§©ã€‚
>
> Weight updates during adaptation have a low "intrinsic rank.

å¯¹äºä»»ä½•é¢„è®­ç»ƒæƒé‡çŸ©é˜µ $W_0 \in \mathbb{R}^{d \times k}$ï¼ŒLoRA å°†æƒé‡æ›´æ–° $\Delta W$ è¡¨ç¤ºä¸ºä½ç§©åˆ†è§£ï¼š
$$
W = W_0 + \Delta W = W_0 + BA
$$
å…¶ä¸­ $B \in \mathbb{R}^{d \times r}$ å’Œ $A \in \mathbb{R}^{r \times k}$ çš„ç§© $r \ll \min(d,k)$ã€‚

åœ¨è®­ç»ƒæœŸé—´ï¼ŒåŸå§‹æƒé‡ $W_0$ ä¿æŒå†»ç»“ï¼Œåªè®­ç»ƒå°å¾—å¤šçš„çŸ©é˜µ $A$ å’Œ $B$ã€‚å‰å‘ä¼ æ’­å˜ä¸ºï¼š
$$
h = W_0x + \Delta Wx = W_0x + BAx
$$
å…¶ä¸­ï¼š 

- çŸ©é˜µ $A$ ç”¨éšæœºé«˜æ–¯å€¼åˆå§‹åŒ–ï¼Œè€Œ $B$ åˆå§‹åŒ–ä¸ºé›¶ï¼Œç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶ $\Delta W = 0$
- å¯¹ $BAx$ åº”ç”¨ç¼©æ”¾å› å­ $\frac{\alpha}{r}$ï¼Œä»¥åœ¨æ”¹å˜ç§© $r$ æ—¶å‡å°‘è¶…å‚æ•°æ•æ„Ÿæ€§



## Reference

[^1]: [Qwen3 X ModelScopeå·¥å…·é“¾: é£é€Ÿè®­ç»ƒ + å…¨é¢è¯„æµ‹](https://mp.weixin.qq.com/s/VopxIcPOc4sQRthxYGVfyw)
[^2]: [xming521/WeClone: æ¬¢è¿starâ­ã€‚ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æœ‰â€œé‚£å‘³å„¿â€ï¼Œå¹¶ç»‘å®šåˆ°èŠå¤©æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«ã€‚ æ•°å­—å…‹éš†/æ•°å­—åˆ†èº«/æ•°å­—æ°¸ç”Ÿ/å£°éŸ³å…‹éš†/LLM/å¤§è¯­è¨€æ¨¡å‹/å¾®ä¿¡èŠå¤©æœºå™¨äºº/LoRA](https://github.com/xming521/WeClone)

[^3]: [hiyouga/EasyR1: EasyR1: An Efficient, Scalabl](https://github.com/hiyouga/EasyR1)
[^4]: [LLM Post-Training: A Deep Dive into Reasoning Large Language Models | alphaXiv](https://www.alphaxiv.org/abs/2502.21321)

[^5]: [A Survey on Post-training of Large Language Models | alphaXiv](https://www.alphaxiv.org/abs/2503.06072)
[^6]: [[2106.09685\] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

